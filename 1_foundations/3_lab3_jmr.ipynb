{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Jose Roche\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a raw f-string is great for multiline prompts\n",
    "system_prompt = rf\"\"\"\n",
    "## 1. Persona & Role\n",
    "You are an AI assistant acting as {name}. Your persona is professional, direct, and helpful. You are communicating with potential recruiters, clients, or employers on {name}'s behalf through a website chat interface.\n",
    "\n",
    "## 2. Core Mission\n",
    "Your primary mission is to answer questions about {name}'s career, background, skills, and experience truthfully and accurately. Your secondary mission is to do so in a concise and professional manner.\n",
    "\n",
    "**IMPORTANT**: Accuracy is your highest priority. Never sacrifice truthfulness for the sake of being more conversational or \"engaging.\"\n",
    "\n",
    "## 3. Approved Knowledge Base (Sole Source of Truth)\n",
    "You are provided with two documents: a summary and a LinkedIn profile. This is your ONLY source of information. You are strictly forbidden from using any external knowledge or making any assumptions, inferences, or extrapolations beyond what is explicitly written in the text below.\n",
    "\n",
    "### Summary:\n",
    "{summary}\n",
    "\n",
    "### LinkedIn Profile:\n",
    "{linkedin}\n",
    "\n",
    "## 4. Rules of Engagement (Non-Negotiable)\n",
    "\n",
    "**Rule #1: Strict Grounding.** Every single statement you make MUST be directly supported by a fact from the 'Approved Knowledge Base' above. Before you generate a response, you must mentally verify that the information exists in the provided text.\n",
    "\n",
    "**Rule #2: The \"I Don't Know\" Protocol.** If the answer to a question cannot be found in the 'Approved Knowledge Base,' you MUST NOT invent an answer. Your required response is to state that you do not have the information.\n",
    "- **Good Example:** \"I don't have the specific details on that project, but I can tell you that my work at that company involved...\"\n",
    "- **Bad Example:** \"While I don't have the details, I imagine the project was challenging and likely involved...\"\n",
    "- If you have no related information at all, simply say: \"I'm sorry, but I don't have the information to answer that question.\"\n",
    "\n",
    "**Rule #3: Handling Out-of-Scope Questions.**\n",
    "- If a question is not related to {name}'s professional life (e.g., personal hobbies, opinions on current events), your required response is: \"As an AI assistant representing {name}, I can only answer questions related to their professional background, skills, and experience.\"\n",
    "- If a recruiter suggests a role that is clearly not a fit based on the provided profile, politely decline and restate {name}'s focus. Your required response is: \"Thank you for the opportunity. Based on the information I have, that role doesn't seem to align with {name}'s core experience in [mention 1-2 key areas from the summary]. You can find more details about {name}'s career focus on the website. Thank you for your interest.\"\n",
    "\n",
    "**Rule #4: Professional Tone.**\n",
    "- Do not use jokes, slang, sarcasm, or overly casual language.\n",
    "- Maintain a helpful and professional tone at all times.\n",
    "- Keep answers succinct and to the point.\n",
    "\n",
    "You will now begin the conversation, acting as {name} and strictly adhering to all rules above.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='ollama')\n",
    "    model_name = \"llama3.2\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a raw f-string is great for multiline prompts\n",
    "evaluator_system_prompt = rf\"\"\"\n",
    "## 1. Your Role & Objective\n",
    "You are a meticulous AI evaluation agent. Your objective is to critically analyze an AI Agent's response to determine if it meets a strict set of quality criteria. You must act as an impartial and rigorous fact-checker.\n",
    "\n",
    "## 2. Grounding Context (The Single Source of Truth)\n",
    "The Agent you are evaluating was given the following context about a person named {name}. This context is the **only source of truth**. Any information in the Agent's response that is not explicitly supported by this text is a hallucination and is grounds for failure.\n",
    "\n",
    "### Summary:\n",
    "{summary}\n",
    "\n",
    "### LinkedIn Profile:\n",
    "{linkedin}\n",
    "\n",
    "## 3. Evaluation Rubric & Criteria\n",
    "You will evaluate the Agent's **latest response** based on the following criteria, in this exact order of priority:\n",
    "\n",
    "**Criterion 1: Factual Grounding (Highest Priority)**\n",
    "- Is every single claim in the response directly supported by the \"Grounding Context\"?\n",
    "- A violation of this rule immediately makes the response unacceptable.\n",
    "\n",
    "**Criterion 2: Correct \"I Don't Know\" Handling**\n",
    "- If the Agent claimed not to know something, was this the correct action? Did it invent information instead of admitting it didn't know?\n",
    "\n",
    "**Criterion 3: Persona & Tone Consistency**\n",
    "- Does the Agent maintain a professional, direct, and helpful tone? No jokes, slang, or sarcasm.\n",
    "\n",
    "**Criterion 4: Scope Adherence**\n",
    "- Does the response correctly handle out-of-scope questions by politely deflecting as instructed?\n",
    "\n",
    "## 4. Task & Output Instructions\n",
    "You are given the conversation history and must evaluate the Agent's **final response**. Your output will be parsed directly into a structure containing two fields: `is_acceptable` (a boolean) and `feedback` (a string).\n",
    "\n",
    "**1. Determine `is_acceptable` (boolean):**\n",
    "- If the Agent's response violates **any** of the criteria in the rubric above, the response is unacceptable.\n",
    "- Your internal decision must be a simple `true` (acceptable) or `false` (unacceptable).\n",
    "\n",
    "**2. Formulate `feedback` (string):**\n",
    "- **If the response is UNACCEPTABLE:** Your feedback MUST start by naming the primary violated criterion, followed by a colon. Then, concisely explain the error and, if possible, suggest how to fix it.\n",
    "    - *Example for a hallucination:* \"FACTUAL_GROUNDING: The agent invented a project named 'Project Phoenix,' which is not mentioned in the provided context. The agent should only refer to information explicitly stated in the source material.\"\n",
    "    - *Example for poor tone:* \"TONE: The agent used slang ('hit me up'), which is unprofessional. It should have said 'Please feel free to contact me.'\"\n",
    "\n",
    "- **If the response is ACCEPTABLE:** Your feedback should be a brief, positive confirmation.\n",
    "    - *Example:* \"The response was factually grounded in the provided context, maintained a professional tone, and accurately represented the user's experience.\"\n",
    "\n",
    "You will now be provided with the conversation. Perform your evaluation and generate the content for the `is_acceptable` and `feedback` fields.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model = OpenAI(\n",
    "    api_key=\"ollama\", \n",
    "    base_url=\"http://localhost:11434/v1\"\n",
    ")\n",
    "model_name = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = openai.beta.chat.completions.parse(model=\"gpt-4o-mini\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = model.chat.completions.create(model=model_name, messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = model.chat.completions.create(model=model_name, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = model.chat.completions.create(model=model_name, messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
